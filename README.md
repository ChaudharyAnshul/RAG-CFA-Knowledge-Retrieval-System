# RAG-CFA-Knowledge-Retrieval-System

## Live application Links
[![codelabs](https://img.shields.io/badge/codelabs-4285F4?style=for-the-badge&logo=codelabs&logoColor=white)](https://codelabs-preview.appspot.com/?file_id=1P5WVoWMA4KMRP6AXZfl4uCWN6v8ejtkE3BhDzUxcbyg#0)

- Airflow (not live): http://35.188.172.153:8080
- Fast API (not live): http://35.224.202.134:8001
- Streamlit Application (not live): http://35.222.74.248:8501

## Problem Statement 
Build an intelligent applications for knowledge retrieval and Q/A tasks.

## Project Goals
1. Creating knowledge summaries using OpenAIâ€™s GPT
2. Generating a knowledge base (Q/A) providing context
3. Using a vector database to find and answer questions.
4. Use the knowledge summaries from 1 to answer questions.

## Technologies Used
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/)
[![Python](https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=blue)](https://www.python.org/)
[![Apache Airflow](https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)](https://airflow.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-%232496ED?style=for-the-badge&logo=Docker&color=blue&logoColor=white)](https://www.docker.com)
[![Google Cloud](https://img.shields.io/badge/Google_Cloud-%234285F4.svg?style=for-the-badge&logo=google-cloud&logoColor=white)](https://cloud.google.com)
[![MongoDB](https://img.shields.io/badge/MongoDB-%234169E1?style=for-the-badge&logo=MongoDB&logoColor=%234169E1&color=black)](https://www.postgresql.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)](https://streamlit.io/)

## Architecture Diagram

<img src="architecture_diagram\architecture diagram.png">


## Pre requisites
1. Python Knowledge
2. Pinecone API Key
3. Openai API Key
4. Docker Desktop
5. MongoDB database knowledge
6. Vector database knowledge
8. Stremlit implementation
9. Airflow pipeline knowledge
10. Google Cloud Platform account and hosting knowledge

```
ðŸ“¦ 
â”œâ”€Â .gitignore
â”œâ”€Â README.md
â”œâ”€Â airflow
â”‚Â Â â”œâ”€Â Dockerfile
â”‚Â Â â”œâ”€Â dags
â”‚Â Â â”‚Â Â â”œâ”€Â configuration.properties.example
â”‚Â Â â”‚Â Â â””â”€Â dag_embedding.py
â”‚Â Â â”œâ”€Â docker-compose.yml
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â architecture_diagram
â”‚Â Â â”œâ”€Â Assignment 5 diagram.png
â”‚Â Â â””â”€Â Assignment_5_flow_diagram.ipynb
â”œâ”€Â cloudfunction_generateMarkdown
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â cloudfunction_generateMarkdownEmbedding
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â cloudfunction_generateQuestioEmbedding
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â fastapi_auth
â”‚Â Â â”œâ”€Â Dockerfile
â”‚Â Â â”œâ”€Â configuration.properties.example
â”‚Â Â â”œâ”€Â docker-compose.yml
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â fastapi_service
â”‚Â Â â”œâ”€Â Dockerfile
â”‚Â Â â”œâ”€Â configuration.properties.example
â”‚Â Â â”œâ”€Â docker-compose.yml
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â”œâ”€Â requirements.txt
â”‚Â Â â””â”€Â routers
â”‚Â Â Â Â Â â”œâ”€Â collection.py
â”‚Â Â Â Â Â â”œâ”€Â questions.py
â”‚Â Â Â Â Â â””â”€Â report.py
â”œâ”€Â script_generateAnswers
â”‚Â Â â”œâ”€Â part3.py
â”‚Â Â â”œâ”€Â part4.py
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â script_generateQuestion
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â script_generateQuestionEmbedding
â”‚Â Â â”œâ”€Â configuration.properties.example
â”‚Â Â â””â”€Â main.py
â”œâ”€Â script_setup
â”‚Â Â â”œâ”€Â configuration.properties.example
â”‚Â Â â”œâ”€Â pdf-extraction.py
â”‚Â Â â”œâ”€Â requirements.txt
â”‚Â Â â””â”€Â webscrape.py
â””â”€Â streamlit_app
Â Â Â â”œâ”€Â Dockerfile
Â Â Â â”œâ”€Â components
Â Â Â â”‚Â Â â”œâ”€Â data_collection.py
Â Â Â â”‚Â Â â”œâ”€Â login_signup.py
Â Â Â â”‚Â Â â”œâ”€Â navigation.py
Â Â Â â”‚Â Â â”œâ”€Â part3_report.py
Â Â Â â”‚Â Â â”œâ”€Â part4_report.py
Â Â Â â”‚Â Â â””â”€Â question_data.py
Â Â Â â”œâ”€Â configuration.properties.example
Â Â Â â”œâ”€Â docker-compose.yml
Â Â Â â”œâ”€Â main.py
Â Â Â â””â”€Â requirements.txt
```
Â©generated by [Project Tree Generator](https://woochanleee.github.io/project-tree-generator)

## How to run Application Locally
1. Clone repository
2. Go to path each folder where requirementss.txt is present.
3. Create configurations.example files where requirements is present and add your respsctive creadentials for
   - AWS S3 bucket:
     Access key = "",
     Secret Key = "",
     Bucket = ""
 
   - MongoDB:
     mongo-username= "",
     mongo-password = "",
     mongo-cluster = ""
 
   - Airflow:
    airflow_un = "",
    airflow_pas = ""
  - Pinecone 

5. Create code env and activate it.
6. run pip install -r requirements.txt
7. Since each service is dockerised, just run docker compose up --build


## References

- https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#sort=%40refreadingcurriculumyear%20descending
- https://pypdf.readthedocs.io/en/stable/
- https://diagrams.mingrammer.com/
- https://airflow.apache.org/
- https://openai.com/blog/openai-api
